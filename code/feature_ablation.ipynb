{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7a7382eb",
   "metadata": {},
   "source": [
    "# Feature ablation with LinearSVC\n",
    "\n",
    "This notebook provides code for feature ablation for SVM. In this step, we have already trained our baseline model, LogisticRegression(), on a few features: `Token`, `Pos`, `Cap_after_lower`, and `Allcaps`. The latter two are binary features which check whether the word is capitalized and comes after a lowercase word, and whether the word is in uppercase, respectively. More features have been extracted initially during the preprocessing phase, during which the conll format has been updated to include more features that have been extracted.\n",
    "\n",
    "In this notebook, we will be using the already processed conll format, so we already have them in the file and they are accessible. For feature ablation, we will take the highest performing model - SVM - and perform feature ablation as follows:\n",
    "1. First I will train SVM on the token only and see if it performs better or worse.\n",
    "2. I will then add back the baseline features -`Pos`, `Cap_after_lower` and `Allcaps`, and see how SVM measures against the basline model with the same features, and how it measures against itself with only the token as the feature.\n",
    "3. I will then add the other orthographic features which are not related to capitalization - `Comp_suf` (followed by a company marker), `Demonym` (contains an adjectivial inflection), and `Poss_mark` (followed by 's) to the token - and see what happens. I will add each feature once, and then do a version that contains all the orthography features together.\n",
    "4. Finally, I will add the `Pos` and `Chunk` to the token to see the extent that syntactical features affect the result.\n",
    "\n",
    "The logic of this order is due to time constraints which prevent me from checking all possible combinations. What I am interested in is the effect of orthographic features that are not related to capitalization, versus those that are. I am also interested in checking a feature set that does not contain any syntactical features.\n",
    "\n",
    "Finally, I will take the best performing combination and test it on LogisticRegression and NaiveBayes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b902154",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "import feature_extraction_util as extract\n",
    "import classification_util as classify\n",
    "import evaluation_util as evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6960c77",
   "metadata": {},
   "source": [
    "# Feature Ablation - eliminating and adding features\n",
    "\n",
    "First I try with eight different combinations. I begin with removing all features and incrementally adding back the orthographic features to the token, compare with the baseline features and check combinations of orthographic and syntactic features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe48cb21",
   "metadata": {},
   "source": [
    "#### The cell below contains necessary definitions in order to train the models, as well as classify the test-sets with pretrained models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "443d8674",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_only = ['Token']\n",
    "baseline_features = ['Token', 'POS','Allcaps', 'Cap_after_lower']\n",
    "token_and_demonym = ['Token', 'Demonym']\n",
    "token_and_comp_suf = ['Token', 'Comp_suf']\n",
    "token_and_poss_mark = ['Token', 'Poss_mark']\n",
    "orthography_only = ['Token', 'Demonym', 'Comp_suf', 'Poss_mark']\n",
    "orthography_and_cap = ['Token', 'Allcaps', 'Cap_after_lower', 'Demonym', 'Comp_suf', 'Poss_mark']\n",
    "token_and_syntax = ['Token', 'POS', 'Chunk']\n",
    "\n",
    "all_selected_features = {'token_only':token_only,'baseline_features':baseline_features,\n",
    "                     'token_and_demonym':token_and_demonym,'token_and_comp_suf':token_and_comp_suf,\n",
    "                     'token_and_poss_mark':token_and_poss_mark,'orthography_only':orthography_only,\n",
    "                     'orthography_and_cap':orthography_and_cap,'token_and_syntax':token_and_syntax}\n",
    "\n",
    "train_file = '../data/conll2003.train.preprocessed.conll'\n",
    "test_file = '../data/conll2003.test.preprocessed.conll'\n",
    "outputfile = '../data/conll2003.test.output.conll' # generic pathname for saving results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4af22bfb",
   "metadata": {},
   "source": [
    "## Extracting the features, training and saving the model\n",
    "\n",
    "If this cell has already been executed, the trained model is loaded in the next cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae873993",
   "metadata": {},
   "outputs": [],
   "source": [
    "for name,selected_features in all_selected_features.items():\n",
    "    \n",
    "    # extract features and create classifier\n",
    "    features, labels = extract.features_and_labels(train_file,selected_features)\n",
    "    model, vec = classify.create_classifier(features,labels,'SVM')\n",
    "    \n",
    "    # save trained model and vectorizer\n",
    "    classifier_pathname = '../models/%s_svm_model_conll2003.sav' % name\n",
    "    vectorizer_pathname = '../models/%s_svm_vec_conll2003.sav' % name\n",
    "    \n",
    "    pickle.dump(model, open(classifier_pathname,'wb')) \n",
    "    pickle.dump(vec, open(vectorizer_pathname,'wb')) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aee42b7d",
   "metadata": {},
   "source": [
    "## Classifying the test sets with the saved model and evaluating the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7be4aa52",
   "metadata": {},
   "outputs": [],
   "source": [
    "for name,selected_features in all_selected_features.items():\n",
    "            \n",
    "    # load saved models and vecs\n",
    "    classifier_pathname = '../models/%s_svm_model_conll2003.sav' % name\n",
    "    vectorizer_pathname = '../models/%s_svm_vec_conll2003.sav' % name\n",
    "    \n",
    "    loaded_model = pickle.load(open(classifier_pathname,'rb'))\n",
    "    loaded_vec = pickle.load(open(vectorizer_pathname,'rb'))\n",
    "        \n",
    "    # classify data and write to file\n",
    "    classify.classify_data(loaded_model,loaded_vec,selected_features,test_file,\n",
    "                           outputfile.replace('.conll','.' + name + '_svm.conll'))\n",
    "    \n",
    "    outputdata = '../data/conll2003.test.output.%s_svm.conll' % name\n",
    "    \n",
    "    # print confusion matrix and classification report\n",
    "    print(\"Classification Report and Confusion Matrix for the %s SVM model\" % name)\n",
    "    evaluate.get_confusion_matrix_and_classification_report(outputdata,exclude_majority=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "904ee5dc",
   "metadata": {},
   "source": [
    "# Checking more feature combinations with LinearSVC\n",
    "\n",
    "Evidently, the feature combination of `Token`, `POS`, and `Chunk` has the best performance in terms of macro precision, recall and f1-score, as well as overall accuracy. I will now try this feature combination together with the orthographic features `Allcaps` and `Cap_after_lower`, which seem to have better results than the other orthographic features. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d59dafb7",
   "metadata": {},
   "source": [
    "## Extracting the features, training and saving the model\n",
    "\n",
    "If this cell has already been executed, the trained model is loaded in the next cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "727f2f5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_syntax_and_cap = ['Token', 'POS', 'Chunk', 'Allcaps', 'Cap_after_lower']\n",
    "\n",
    "# extract features and labels\n",
    "features, labels = extract.features_and_labels(train_file,token_syntax_and_cap)\n",
    "\n",
    "# create SVM classifier\n",
    "model, vec = classify.create_classifier(features,labels,'SVM')\n",
    "\n",
    "# save trained model and vectorizer\n",
    "classifier_pathname = '../models/token_syntax_and_cap_svm_model_conll2003.sav'\n",
    "vectorizer_pathname = '../models/token_syntax_and_cap_svm_vec_conll2003.sav'\n",
    "\n",
    "pickle.dump(model, open(classifier_pathname,'wb'))\n",
    "pickle.dump(vec, open(vectorizer_pathname,'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad8eb0d0",
   "metadata": {},
   "source": [
    "## Classifying the test sets with the saved models and evaluating the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7db4807",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load saved model and vectorizer \n",
    "\n",
    "classifier_pathname = '../models/token_syntax_and_cap_svm_model_conll2003.sav'\n",
    "vectorizer_pathname = '../models/token_syntax_and_cap_svm_vec_conll2003.sav'\n",
    "\n",
    "loaded_model = pickle.load(open(classifier_pathname,'rb'))\n",
    "loaded_vec = pickle.load(open(vectorizer_pathname,'rb'))\n",
    "\n",
    "# classify data and write to file\n",
    "classify.classify_data(loaded_model,loaded_vec,token_syntax_and_cap,test_file, \n",
    "                       outputfile.replace('.conll','.token_syntax_and_cap_svm.conll'))\n",
    "\n",
    "outputdata = '../data/conll2003.test.output.token_syntax_and_cap_svm.conll'\n",
    "\n",
    "# print confusion matrix and classification report\n",
    "print(\"Classification Report and Confusion Matrix for the token_syntax_and_cap SVM model\")\n",
    "evaluate.get_confusion_matrix_and_classification_report(outputdata,exclude_majority=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05e8109c",
   "metadata": {},
   "source": [
    "# Trying the best feature combination with LogisticRegression and NaiveBayes\n",
    "\n",
    "I will now try the feature combination of `Token`, `POS`, and `Chunk`, together with the orthographic features `Allcaps` and `Cap_after_lower`, with other classifiers - namely LogisticRegression and NaiveBayes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e042ee2",
   "metadata": {},
   "source": [
    "## Extracting the features, training and saving the models\n",
    "\n",
    "If this cell has already been executed, the trained model is loaded in the next cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9b3369a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for modelname in ['logreg', 'NB']:\n",
    "    \n",
    "    # extract features and create classifiers\n",
    "    features, labels = extract.features_and_labels(train_file,token_syntax_and_cap)\n",
    "    model, vec = classify.create_classifier(features,labels,modelname)\n",
    "    \n",
    "    # save trained model and vectorizer\n",
    "    classifier_pathname = '../models/token_syntax_and_cap_%s_model_conll2003.sav' % modelname\n",
    "    vectorizer_pathname = '../models/token_syntax_and_cap_%s_vec_conll2003.sav' % modelname\n",
    "\n",
    "    pickle.dump(model, open(classifier_pathname, 'wb'))\n",
    "    pickle.dump(vec, open(vectorizer_pathname,'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae2f9652",
   "metadata": {},
   "source": [
    "## Classifying the test sets with the saved models and evaluating the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b4a5c32",
   "metadata": {},
   "outputs": [],
   "source": [
    "for modelname in ['logreg', 'NB']:\n",
    "    \n",
    "    classifier_pathname = '../models/token_syntax_and_cap_%s_model_conll2003.sav' % modelname\n",
    "    vectorizer_pathname = '../models/token_syntax_and_cap_%s_vec_conll2003.sav' % modelname\n",
    "    \n",
    "    loaded_model = pickle.load(open(classifier_pathname,'rb'))\n",
    "    loaded_vec = pickle.load(open(vectorizer_pathname,'rb'))\n",
    "    \n",
    "    # classify data and write to file\n",
    "    classify.classify_data(loaded_model,loaded_vec,token_syntax_and_cap,test_file, \n",
    "                           outputfile.replace('.conll','.token_syntax_and_cap_' + modelname + '.conll'))\n",
    "                \n",
    "    outputdata = '../data/conll2003.test.output.token_syntax_and_cap_%s.conll' % modelname\n",
    "    \n",
    "    # print confusion matrix and classification report\n",
    "    print(\"Classification Report and Confusion Matrix for the token_syntax_and_cap %s model\" % modelname)\n",
    "    evaluate.get_confusion_matrix_and_classification_report(outputdata,exclude_majority=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
