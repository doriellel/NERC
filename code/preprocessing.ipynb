{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing and editing the conll format\n",
    "\n",
    "This notebook provides code for precprocessing and feature extraction. After applying this code, the conll file is edited to include columns for each additional feature. This only needs to be run once, at the beginning of the experiment. This has already been run, but you can run it again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature extraction Functions\n",
    "\n",
    "def all_caps(dataframe):\n",
    "    '''\n",
    "    adds a column containing TRUE or FALSE values for the Allcaps feature\n",
    "    Allcaps checks whether the token is in allcaps\n",
    "    \n",
    "    :param dataframe: a dataframe containing the conll2003 data\n",
    "    :type dataframe: pandas.core.frame.DataFrame\n",
    "    \n",
    "    returns an updated dataframe\n",
    "    '''\n",
    "    checklist = []\n",
    "\n",
    "    all_caps = r\"^[A-Z][A-Z]+$\"\n",
    "    \n",
    "    for token in dataframe['Token']:\n",
    "        \n",
    "        if re.match(all_caps,token):\n",
    "            checklist.append('TRUE')\n",
    "        else:\n",
    "            checklist.append('FALSE') \n",
    "                                \n",
    "    dataframe['Allcaps'] = checklist\n",
    "    \n",
    "    return dataframe\n",
    "\n",
    "def capitalization_after_lowercase(dataframe):\n",
    "    '''\n",
    "    adds a column containing TRUE or FALSE values for the Cap_after_lower feature\n",
    "    Cap_after_lower checks whether the token is capitalized and comes after an all-lowercase token\n",
    "    \n",
    "    :param dataframe: a dataframe containing the conll2003 data\n",
    "    :type dataframe: pandas.core.frame.DataFrame\n",
    "    \n",
    "    returns an updated dataframe\n",
    "    '''\n",
    "    checklist = []\n",
    "\n",
    "    starts_with_cap = r\"^[A-Z][a-z]+\"\n",
    "    all_lowercase = r\"^[a-z]+\"\n",
    "    previous_token = \"\"\n",
    "    \n",
    "    for token in dataframe['Token']:\n",
    "        \n",
    "        if re.match(starts_with_cap,token) and re.match(all_lowercase,previous_token):\n",
    "            checklist.append('TRUE')\n",
    "        else:\n",
    "            checklist.append('FALSE') \n",
    "                        \n",
    "        previous_token = token\n",
    "        \n",
    "    dataframe['Cap_after_lower'] = checklist\n",
    "    \n",
    "    return dataframe\n",
    "\n",
    "def origin_adjective_suffix(dataframe):\n",
    "    '''\n",
    "    adds a column containing TRUE or FALSE values for the Demonym feature\n",
    "    Demonym checks whether the token contains a suffix following the origin adjectivial pattern\n",
    "    \n",
    "    :param dataframe: a dataframe containing the conll2003 data\n",
    "    :type dataframe: pandas.core.frame.DataFrame\n",
    "    \n",
    "    returns an updated dataframe\n",
    "    '''\n",
    "    checklist = []\n",
    "    origin_adjective_suffixes = ['ian','ese','ish','ean','i']\n",
    "    list_of_tokens = dataframe['Token'].to_list()\n",
    "    \n",
    "    for token in dataframe['Token']:\n",
    "        \n",
    "        checker = []\n",
    "        \n",
    "        for suffix in origin_adjective_suffixes:\n",
    "            \n",
    "            if token.endswith(suffix):\n",
    "                checker.append('TRUE')\n",
    "            else:\n",
    "                checker.append('FALSE')\n",
    "        \n",
    "        if 'TRUE' in checker:\n",
    "            checklist.append('TRUE')\n",
    "        else:\n",
    "            checklist.append('FALSE')\n",
    "        \n",
    "    dataframe['Demonym'] = checklist\n",
    "    \n",
    "    return dataframe\n",
    "\n",
    "def followed_by_company_suffix_check(dataframe):\n",
    "    '''\n",
    "    adds a column containing TRUE or FALSE values for the Comp_suf feature\n",
    "    Comp_suf checks whether the token is followed by a token in the form of a company marker, e.g. Ltd, Inc\n",
    "    \n",
    "    :param dataframe: a dataframe containing the conll2003 data\n",
    "    :type dataframe: pandas.core.frame.DataFrame\n",
    "    \n",
    "    returns an updated dataframe\n",
    "    '''\n",
    "    checklist = []\n",
    "    company_suffix_list = ['Ltd','Ltd.','Inc.','Corp.','LLC','Co']\n",
    "    list_of_tokens = dataframe['Token'].to_list()\n",
    "    next_token = \"\"\n",
    "    \n",
    "    for token in reversed(list_of_tokens):\n",
    "                \n",
    "        if next_token in company_suffix_list:\n",
    "            checklist.append('TRUE')\n",
    "        else:\n",
    "            checklist.append('FALSE')\n",
    "            \n",
    "        next_token = token\n",
    "    \n",
    "    dataframe['Comp_suf'] = list(reversed(checklist))\n",
    "    \n",
    "    return dataframe\n",
    "\n",
    "def followed_by_possessive_marker(dataframe):\n",
    "    '''\n",
    "    adds a column containing TRUE or FALSE values for the Poss_mark feature\n",
    "    Poss_mark checks whether the token is followed by the token 's, indicating possession\n",
    "    \n",
    "    :param dataframe: a dataframe containing the conll2003 data\n",
    "    :type dataframe: pandas.core.frame.DataFrame\n",
    "    \n",
    "    returns an updated dataframe\n",
    "    '''\n",
    "    checklist = []\n",
    "    possessive_marker = \"'s\"\n",
    "    list_of_tokens = dataframe['Token'].to_list()\n",
    "    next_token = \"\"\n",
    "    \n",
    "    for token in reversed(list_of_tokens):\n",
    "        \n",
    "        if next_token == possessive_marker:\n",
    "            checklist.append('TRUE')\n",
    "        else:\n",
    "            checklist.append('FALSE')\n",
    "            \n",
    "        next_token = token\n",
    "    \n",
    "    dataframe['Poss_mark'] = list(reversed(checklist))\n",
    "    \n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions that update the dataframe with the features and write it back to the file\n",
    "\n",
    "def update_dataframe_with_features(dataframe):\n",
    "    '''\n",
    "    adds columns of extracted feature values to dataframe\n",
    "    \n",
    "    :param dataframe: a dataframe containing the conll2003 data\n",
    "    :type dataframe: pandas.core.frame.DataFrame\n",
    "    \n",
    "    returns an updated dataframe\n",
    "    '''\n",
    "    dataframe = all_caps(dataframe)\n",
    "    dataframe = capitalization_after_lowercase(dataframe)\n",
    "    dataframe = origin_adjective_suffix(dataframe)\n",
    "    dataframe = followed_by_company_suffix_check(dataframe)\n",
    "    dataframe = followed_by_possessive_marker(dataframe)\n",
    "    \n",
    "    return dataframe\n",
    "\n",
    "def update_conll_format(inputfile):\n",
    "    '''\n",
    "    rewrites inputfile to include the extracted features\n",
    "    \n",
    "    :param inputfile: path to a file containing the conll2003 data\n",
    "    :type inputfile: string\n",
    "    \n",
    "    '''\n",
    "    # converting csv file to data frame\n",
    "    df = pd.read_csv(inputfile, sep='\\t',\n",
    "                         names=['Token', 'POS', 'Chunk', 'Gold'])\n",
    "    df = df.dropna() # drop rows that contains nan values\n",
    "    \n",
    "    # adding features to the dataframe\n",
    "    updated_df = update_dataframe_with_features(df)\n",
    "    \n",
    "    #rewriting trainingfile to include updated features\n",
    "    updated_df.to_csv(inputfile.replace('.conll','.preprocessed.conll'), sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing the CONLL files\n",
    "\n",
    "Below I apply the preprocessing functions to the three CONLL files. This has already been applied, and I am using the preprocessed files in the rest of the notebooks, but you can run this again and it will simply rewrite the files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "update_conll_format('../data/conll2003.train.conll')\n",
    "update_conll_format('../data/conll2003.dev.conll')\n",
    "update_conll_format('../data/conll2003.test.conll')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
